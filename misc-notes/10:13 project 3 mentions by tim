10/13 project 3 mentions by tim

per https://www.pythonpool.com/python-convert-unix-time-to-datetime/

import datetime
 
unixToDatetime = datetime.datetime.fromtimestamp(1661324766) # Unix Time
print(unixToDatetime)



So the Reddit API as we're going to be using it will only allow you to pull 1,000 posts at a time.

And depending on your subreddit, you might not actually get it 1,000. You might get like in the 9 hundreds or you might go way less and we do require you to have at least a thousand for your project.

But also for an NLP project, a thousand is probably just not enough to do the cool stuff you want to do.

But Reddit is updating all the time people are making new posts and things like that. So a really smart thing to do would be to run your script that pulls data.

Every day and have some way of archiving your data and dropping duplicates. 

So let's say you pull a thousand posts today.
And then if 1,000 posts tomorrow. And 800 of those are duplicates, but the last 200 were the 200 new ones that they made.
06:41:30
	So pull them every day and then drop duplicates. That will get you well above a thousand.


	The, yeah, I, I would just, based on everything. The idea is that if you do 2 days in a row, a lot of the posts are literally just going to be the same post.
	06:44:05
	So it'll have the same created UTC title, self-text and subreddit.


06:41:37
But of course that means that you're going to have to pull. Posts as soon as possible.




Gotcha. Okay. And If you happen to know off the top of your head, does, retrieving posts by Date range?
06:44:21
I wish the API we used to use for this allowed you to do that. And they were one of the casualties of the great Reddit purge.






