{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db427d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np #tim always does (for .arange() ??)\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3e56be-a4bf-43d4-8c57-7f9954b44b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#well I already did my pulls via the other script, \n",
    "    #and cat >>ed them into a largefile.txt (illegal for rubric)\n",
    "\n",
    "\n",
    "\n",
    "#1- first, remove the scraper code from this file\n",
    "    #bc that's done externally anyway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103d7151-cf2e-47cb-82c2-578faa763888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for future reference, here are my subreddits:\n",
    "#sub1reddit = reddit.subreddit('explainlikeimfive')\n",
    "#sub2reddit = reddit.subreddit('nostupidquestions')\n",
    "\n",
    "\n",
    "#2 - \n",
    "#read in *.csv PER SUBREDDIT, to get 2 separate ENORMOUS DFs (1 per subr)\n",
    "    #eg parse the filename sub1/2 into the right DF\n",
    "    #rename columns for the new DF as needed to ensure no confusion later when joined\n",
    "        #eg: 'self_text' -> 'eli5_self_text'\n",
    "        \n",
    "#this had come from a pull\n",
    "#df_sub1 = pd.DataFrame(data, columns = ['created_utc', 'title', 'self_text', 'subreddit'])\n",
    "\n",
    "#but, planned new code == I'll be reading from csv's \n",
    "\n",
    "#new code goes here, haha\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9197060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.0 - remove the empty posts, wth\n",
    "#df_sub1[ df_sub1['self_text'] != ''].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1 then de-dupe those DFs above\n",
    "    #- TIP: You can use the created_utc attribute of a post to keep track of the timestamp and ensure non-overlapping pulls. The created_utc attribute represents the post's creation time in UTC.\n",
    "    # this may have saved me the de-duping work\n",
    "    \n",
    "    \n",
    "\n",
    "#attrib Mike:\n",
    "# Yeah, I'm also not sure if this will help anyone, right? Miss the boat on this because it can't get a little late as well but I was able to find that if you look at the documentation on this, and the submission.\n",
    "# Reddit object that has more properties than what they have in the initial notebook for us. And there's a very convenient just unique ID that you can just pull in.\n",
    "\n",
    "\n",
    "#attrib: Rafi \n",
    "#moviereviews.duplicated().sum()\n",
    "    #check if dupes\n",
    "#moviereviews.drop_duplicates(inplace=True)\n",
    "    #drop if so\n",
    "    \n",
    "#3.9 - write each trimmed DF to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 join both subreddits' DFs, \n",
    "#within a new dataframe\n",
    "    #why? we were told to? \n",
    "    #per the example code:\n",
    "    # \"Eventually, you will want to combine your two dataframes together  \"to do modelling\"\"\n",
    "\n",
    "#4.9 - write the combo DF to a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb4c19d-f9d9-4be8-9ca5-5ecbbde48970",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09636020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67297667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 EDA per tutorial\n",
    "#TODO Once you have at least 1000 posts from each subreddit\n",
    "#, you can do some EDA \n",
    "#5.1 perhaps maybe the most common words in each subreddit..?\n",
    "    #see the NLP lesson &/or lab\n",
    "#\n",
    "#5.2 look for patterns \n",
    "    #compare the most-common from each\n",
    "    #least-common?\n",
    "    #length of title?\n",
    "    #length of post?\n",
    "#5.2.1 - pairplot?\n",
    "    #google this, but histo the common words (if they match?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851586d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb4a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bbef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppt - \n",
    "    #make this early on Wednesday? 1st thing Thursday night?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bcc636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob stmt thoughts:\n",
    "    #attrib Katie:\n",
    "    #goal: doesn't have to be online/marketing\n",
    "    #instead, could be email redirects to diff depts w/in company\n",
    "    #eg library reference depts, or the equiv. of same but chatAI avatars\n",
    "    #\n",
    "    #even, we could just say \"look, here's a model, I can do it\"\n",
    "        #for somebody who wants any modeling later\n",
    "        #it is proof of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo - rename / make new notebook(s)\n",
    "#todo - last - save to script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
